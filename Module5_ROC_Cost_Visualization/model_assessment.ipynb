{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "model_assessment.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlkzIec5NqTY"
      },
      "source": [
        "# Model Assessment\n",
        "\n",
        "\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6JiPDcUNqTZ"
      },
      "source": [
        "Import all of the packages we will need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "D2Njx_uKNqTZ"
      },
      "source": [
        "# Import the libraries we will be using\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Our custom libraries!\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "! git clone https://github.com/yizuc/datamining.git\n",
        "from datamining.ds_utils.decision_surface import *\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = 15, 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5QpZ0sANqTa"
      },
      "source": [
        "### Data\n",
        "We're going to use a mail response data set from a real direct marketing campaign located in `data/mailing.csv`. Each record represents an individual who was targeted with a direct marketing offer.  The offer was a solicitation to make a charitable donation. \n",
        "\n",
        "The columns (features) are:\n",
        "\n",
        "```\n",
        "income       household income\n",
        "Firstdate    data assoc. with the first gift by this individual\n",
        "Lastdate     data associated with the most recent gift \n",
        "Amount       average amount by this individual over all periods (incl. zeros)\n",
        "rfaf2        frequency code\n",
        "rfaa2        donation amount code\n",
        "pepstrfl     flag indicating a star donator\n",
        "glast        amount of last gift\n",
        "gavr         amount of average gift\n",
        "```\n",
        "\n",
        "The target variables is `class` and is equal to one if they gave in this campaign and zero otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOGRojqoyack",
        "outputId": "88b4946b-e52f-452b-8c46-5eade9f567e8"
      },
      "source": [
        "%cd /content/datamining/Module5_ROC_Cost_Visualization\n",
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datamining/Module5_ROC_Cost_Visualization\n",
            "data  images  model_assessment.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "qxCrRF2RNqTa"
      },
      "source": [
        "# Load the data\n",
        "data = pd.read_csv(\"data/mailing.csv\")\n",
        "# Let's take a look at the data\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbat_kYrNqTb"
      },
      "source": [
        "From the description above, and the head of the data, we see that two of the fields are **categorical** (text) instead of typical **numerical** fields. Today, one of the models we will be using is a logistic regression. From the previous classes, we have seen that logistic regression requires *all* fields to be numerical. So, we are going to create \"dummy\" variables for all the fields that are categorical (same as you did for your homework).\n",
        "\n",
        "#### Dummyize\n",
        "A dummy variable is a binary variable corresponding to one value of a categorical variable.\n",
        "The typical way to create dummies for a field is to create new variables for each possible category of the field. For example consider a field called color that can have the possible values \"red\", \"blue\", and \"green\". To dummyize color, we would create three new features: \"color_red\", \"color_blue\", and \"color_green\". These fields would take the value 1 or 0 depending on the actual value of color. Each record can only have one of these fields set to 1.\n",
        "\n",
        "Notes:\n",
        "\n",
        "- You can also leave out one of the possible categories. For example, in the above example that had three possible values, you can create only two dummies. This, because when \"color_red\"=0 and \"color_blue\"=0 it means that \"color_green=1\".  Often all three dummies are created anyway; it is slightly redundant, but makes the models more comprehensible.\n",
        "\n",
        "- There also are cases where non-numeric variables can take on multiple values (for example, `colors = {red, white, blue}`).  In these cases again often binary variables are created for each value, the obvious difference being that now more than one can be non-zero (and you would need to represent all the values).\n",
        " \n",
        "\n",
        "So.  Let's dummyize the fields `rfaa2` and `pepstrfl`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ZbisSX03NqTb"
      },
      "source": [
        "for field in ['rfaa2', 'pepstrfl']:\n",
        "    dummies = pd.get_dummies(data[field])\n",
        "    dummies.columns = [field + \"_\" + s for s in dummies.columns]\n",
        "    data = pd.concat([data, dummies], axis=1).drop(field, axis=\"columns\")\n",
        "    \n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4-FQj8XNqTc"
      },
      "source": [
        "### Confusion matrices\n",
        "Let's build a confusion matrix using a logistic regression model. \n",
        "\n",
        "**Important and overlooked (always remember this!):** a confusion matrix is defined with respect to a classifier, not a scoring model (e.g., a class-probability estimation model).  Our models *are* scoring models.  So the confusion matrices are defined with respect a scoring model plus a *threshold* on the score.  The threshold should be chosen carefully, and with the business need in mind.   For binary classes, the default for most modeling programs when they return a predicted classification is to use a threshold corresponding to an estimated class probability of 0.5.  This is because the modeling program does not know the business setting, and 0.5 makes sense as a default (in expectation it gives the maximum classification accuracy, if the probabilities are well calibrated).\n",
        "\n",
        "So let's start with the default of predicting a 1 if the estimated probability is $\\geq$ 50% and a 0 otherwise.\n",
        "\n",
        "Remember, a confusion matrix looks like:\n",
        "\n",
        "```\n",
        "  |____________ p __________|___________ n ___________|\n",
        "Y | 1's predicted to be 1's | 0's predicted to be 1's |\n",
        "N | 1's predicted to be 0's | 0's predicted to be 0's |\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "rMx2kT6XNqTc"
      },
      "source": [
        "# Split our data\n",
        "X = data.drop(['class'], axis=1)\n",
        "Y = data['class']\n",
        "X_mailing_train, X_mailing_test, Y_mailing_train, Y_mailing_test = train_test_split(X, Y, test_size=.25, random_state=42)\n",
        "\n",
        "# Make and fit a model on the training data\n",
        "model_mailing = LogisticRegression(C=1000000, solver='liblinear')\n",
        "model_mailing.fit(X_mailing_train, Y_mailing_train)\n",
        "\n",
        "# Get probabilities of being a (We saw this last class !!)\n",
        "probabilities = model_mailing.predict_proba(X_mailing_test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjHMiwujNqTc"
      },
      "source": [
        "Use the default threshold of 50% to predict a 1.\n",
        "\n",
        "(An individual below this threshold will get a label \"0\" and someone above this will get a label \"1\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "jRiCieHbNqTc"
      },
      "source": [
        "prediction = probabilities > 0.5\n",
        "\n",
        "# Build and print a confusion matrix\n",
        "confusion_matrix_large = pd.DataFrame(metrics.confusion_matrix(Y_mailing_test, prediction, labels=[1, 0]).T,\n",
        "                                columns=['p', 'n'], index=['Y', 'N'])\n",
        "print (confusion_matrix_large)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_AVhT39NqTc"
      },
      "source": [
        "Wait -- take a close look at that.  What's going on?\n",
        "\n",
        "Incidentally, what would be the classification accuracy?\n",
        "\n",
        "-\n",
        "\n",
        "-\n",
        "\n",
        "-\n",
        "\n",
        "-\n",
        "\n",
        "-\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT8tvchvNqTd"
      },
      "source": [
        "What if we lower the threshold to 5%?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "gswBhJSGNqTd"
      },
      "source": [
        "# Let's move the threshold down\n",
        "prediction = probabilities > 0.05\n",
        "\n",
        "# Build and print a confusion matrix\n",
        "confusion_matrix_small = pd.DataFrame(metrics.confusion_matrix(Y_mailing_test, prediction, labels=[1, 0]).T,\n",
        "                                columns=['p', 'n'], index=['Y', 'N'])\n",
        "print (confusion_matrix_small)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUlTE3tRNqTd"
      },
      "source": [
        "***\n",
        "Is this good performance? How can we tell?\n",
        "\n",
        "(Incidentally, what would be the classification accuracy now?)\n",
        "\n",
        "Is 5% the right threshold?  How would we determine that?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4XpYSiJNqTd"
      },
      "source": [
        "### Other measures: Receiver operating characteristic (ROC) curves\n",
        "\n",
        "In the book, we were trying to predict if customers should be given a credit card.  Let's use that example here now:\n",
        "\n",
        "- Target: `Y_handson = 1` \n",
        "- Three features in `X_handson`: \"earning\", \"geographic\", and \"experience\".\n",
        "\n",
        "Up until this point, when we need a \"single number metric\" for generalization performance, we have been using \"vanilla\" classification accuracy (the number of records correctly classified divided by the total number of records). However, this does not always give us the \"best\" interpretation of our model's performance for a particular business problem. An alternative way is to visualize and measure the performance of a model using the Reciever Operating Characteristic **(ROC) curve**. \n",
        "\n",
        "Let's first specify how we create them: For each threshold $t$ that is chosen, we can define two quantities. First, the false positive rate, $FPR = \\frac{False Positives}{False Positives+True Negatives}$, and second, the true positive rate, $TPR = \\frac{True Positives}{True Positives+False Negatives}$. The ROC curve is the result of plotting $FPR$ against $TPR$ for each value of $t$ that is possible in the data.  It helps us to visualize and analyze the trade-offs between the opportunity for benefits (via true positives) and the possibility of costs (via false positives). \n",
        "\n",
        "\" The lower left point **(0, 0) represents the strategy of never issuing a positive classification**; such a classifier commits no false positive errors but also gains no true positives. The opposite strategy, of unconditionally issuing positive classifications, is represented by the upper right point (1, 1). The point **(0, 1) represents perfect classification** (the star in Figure 8-3). The diagonal line connecting (0, 0) to (1, 1) represents the policy of guessing a class (for example, by flipping a weighted coin). \"  \n",
        "\n",
        "- _Provost, Foster, and Tom Fawcett. Data Science for Business: _\n",
        "  _What you need to know about data mining and data-analytic thinking. O'Reilly Media, Inc., 2013._\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/yizuc/datamining/blob/master/Module5_ROC_Cost_Visualization/images/ROC1.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "<img src=\"https://github.com/yizuc/datamining/blob/master/Module5_ROC_Cost_Visualization/images/ROC2.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "<img src=\"https://github.com/yizuc/datamining/blob/master/Module5_ROC_Cost_Visualization/images/ROC3.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
        "\n",
        "\n",
        "Doing this in **sklearn** is relatively straightforward.\n",
        "\n",
        "Let's create a **new DATA SET** for this, that will show differences between different models.  We will build logistic regression models with different regularization parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "HJfQ2dsrNqTd"
      },
      "source": [
        "X, Y = get_dummy_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r5_xZ3XNqTd"
      },
      "source": [
        "We can now build and fit a model. Using this model, we will plot a *ROC curve*. \n",
        "\n",
        "Why are we taking the mean from the `cross_validation.cross_val_score` output?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "fsRkrNZFNqTd"
      },
      "source": [
        "#Split train and test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)\n",
        "\n",
        "# Fit a logistic regression model\n",
        "for c in [0.01, 0.05, .1, 1]:\n",
        "    model = LogisticRegression(C=c, solver='liblinear')\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    # Get the probability of Y_test records being = 1\n",
        "    Y_test_probability_1 = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Use the metrics.roc_curve function to get the true positive rate (tpr) and false positive rate (fpr)\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_test_probability_1)\n",
        "    \n",
        "    # Get the area under the curve (AUC)\n",
        "    auc = np.mean(cross_val_score(model, X, Y, scoring=\"roc_auc\", cv=5))\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plt.plot(fpr, tpr, label=\"AUC (C=\" + str(c) + \") = \" + str(round(auc, 2)))\n",
        "    \n",
        "plt.xlabel(\"False positive rate (fpr)\")\n",
        "plt.ylabel(\"True positive rate (tpr)\")\n",
        "plt.plot([0,1], [0,1], 'k--', label=\"Random\")\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVhiUyFuNqTe"
      },
      "source": [
        "### Other measures: Cumulative response and lift curves\n",
        "The interpretation of an ROC curve is not be entirely intuitive to business stakeholders. In many applications the **cumulative response curve** is more appropriate.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "tGg_HEaMNqTe"
      },
      "source": [
        "def build_cumulative_curve(model, scale=100):\n",
        "    # Fit model\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    # Get the probability of Y_test records being = 1\n",
        "    Y_test_probability_1 = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Sort theseprobabilities and the true value in descending order of probability\n",
        "    order = np.argsort(Y_test_probability_1)[::-1]\n",
        "    Y_test_probability_1_sorted = Y_test_probability_1[order]\n",
        "    Y_test_sorted = np.array(Y_test)[order]\n",
        "\n",
        "    # Build the cumulative response curve\n",
        "    x_cumulative = np.arange(len(Y_test_probability_1_sorted)) + 1\n",
        "    y_cumulative = np.cumsum(Y_test_sorted)\n",
        "\n",
        "    # Rescale\n",
        "    x_cumulative = np.array(x_cumulative)/float(x_cumulative.max()) * scale\n",
        "    y_cumulative = np.array(y_cumulative)/float(y_cumulative.max()) * scale\n",
        "    \n",
        "    return x_cumulative, y_cumulative\n",
        "\n",
        "def plot_cumulative_curve(models):\n",
        "    # Plot curve for each model\n",
        "    for key in models:\n",
        "        x_cumulative, y_cumulative = build_cumulative_curve(models[key])\n",
        "        plt.plot(x_cumulative, y_cumulative, label=key)\n",
        "    # Plot other details\n",
        "    plt.plot([0,100], [0,100], 'k--', label=\"Random\")\n",
        "    plt.xlabel(\"Percentage of test instances targeted (decreasing score)\")\n",
        "    plt.ylabel(\"Percentage of positives targeted\")\n",
        "    plt.title(\"Cumulative response curve\")\n",
        "    plt.legend()\n",
        "\n",
        "models = {\"Logistic Regression\": LogisticRegression(C=1.0, solver=\"liblinear\")}\n",
        "plot_cumulative_curve(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJPXCqkZNqTe"
      },
      "source": [
        "We can also easily plot a **lift curve** in this scenario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "IxL_z1aQNqTe"
      },
      "source": [
        "def plot_lift_curve(models):\n",
        "    # Plot curve for each model\n",
        "    for key in models:\n",
        "        x_cumulative, y_cumulative = build_cumulative_curve(models[key])\n",
        "        plt.plot(x_cumulative, y_cumulative/x_cumulative, label=key)\n",
        "    # Plot other details\n",
        "    plt.plot([0,100], [1,1], 'k--', label=\"Random\")\n",
        "    plt.xlabel(\"Percentage of test instances (decreasing score)\")\n",
        "    plt.ylabel(\"Lift (times)\")\n",
        "    plt.title(\"Lift curve\")\n",
        "    plt.legend()\n",
        "\n",
        "plot_lift_curve(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEjwCvnNNqTf"
      },
      "source": [
        "Let's revisit our **mailing dataset** compare the cumulative response curves and then the lift curves of two models (a logistic regression model and a tree)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "hvpO3U4UNqTf"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = X_mailing_train, X_mailing_test, Y_mailing_train, Y_mailing_test\n",
        "\n",
        "models = {\"Logistic Regression\": LogisticRegression(C=1.0, solver=\"liblinear\"), \n",
        "          \"Decision Tree\": DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf=300, random_state=42)}\n",
        "plot_cumulative_curve(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "E7KpVDDmNqTf"
      },
      "source": [
        "plot_lift_curve(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCp73JjsNqTf"
      },
      "source": [
        "What if we want to understand not just lift, but how much benefit we are going to receive from a certain investment in targeting?  We can plot a profit curve.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qiR97fYNqTf"
      },
      "source": [
        "### Profit curves\n",
        "Let's say that each offer costs \\$1 to make and market, and each accepted offer earns \\$18, for a profit of $17. The cost matrix would be:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YPwCZVF-NqTf"
      },
      "source": [
        "unit_cost = 1\n",
        "unit_revenue = 18\n",
        "\n",
        "cost_matrix = pd.DataFrame([[unit_revenue - unit_cost, - unit_cost], [0, 0]], columns=['p', 'n'], index=['Y', 'N'])\n",
        "print (\"Cost matrix\")\n",
        "print (cost_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Te-T7CBNqTf"
      },
      "source": [
        "Remember that we examined different targeting thresholds: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Epjpp4YnNqTg"
      },
      "source": [
        "print (\"Confusion matrix with threshold = 50% to predict labels\")\n",
        "print (confusion_matrix_large)\n",
        "print (\"\\n\")\n",
        "print (\"Confusion matrix with threshold = 5% to predict labels\")\n",
        "print (confusion_matrix_small)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIVLkHV_NqTg"
      },
      "source": [
        "\n",
        "Based on those predictions, the expected profit of using 50% and 5% as your prediction threshold would be.\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "qXUPGVkMNqTg",
        "outputId": "0dad7b74-e983-4f03-847d-0f2a9909c4f4"
      },
      "source": [
        "profit_in_large = np.sum((confusion_matrix_large * cost_matrix).values)\n",
        "profit_in_small = np.sum((confusion_matrix_small * cost_matrix).values)\n",
        "\n",
        "print (\"Expected profit per targeted individual with a cutoff of 50%% is $%.2f.\" % profit_in_large)\n",
        "print (\"Expected profit per targeted individual with a cutoff of 5%% is $%.2f.\" % profit_in_small)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected profit per targeted individual with a cutoff of 50% is $-1.00.\n",
            "Expected profit per targeted individual with a cutoff of 5% is $4210.00.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZP1PlIlNqTg"
      },
      "source": [
        "The following would be a profit curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsI-16z3NqTg"
      },
      "source": [
        "unit_cost = -cost_matrix['n']['Y']\n",
        "unit_revenue = cost_matrix['p']['Y'] + unit_cost\n",
        "\n",
        "def plot_profit_curve(models):\n",
        "    # Plot curve for each model\n",
        "    total_obs = len(Y_test)\n",
        "    total_pos = Y_test.sum()\n",
        "    for key in models:\n",
        "        x_cumulative, y_cumulative = build_cumulative_curve(models[key], scale=1)\n",
        "        profits = unit_revenue * y_cumulative * total_pos - unit_cost * x_cumulative * total_obs\n",
        "        plt.plot(x_cumulative*100, profits, label=key)\n",
        "    # Plot other details\n",
        "    plt.xlabel(\"Percentage of users targeted\")\n",
        "    plt.ylabel(\"Profit\")\n",
        "    plt.title(\"Profit curve\")\n",
        "    plt.legend()\n",
        "    \n",
        "plot_profit_curve(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tr7X2CTNqTg"
      },
      "source": [
        "Which one do you think we should choose? Why?"
      ]
    }
  ]
}