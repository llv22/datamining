{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "Supervised segmentation.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYUfwfP8CdZC"
      },
      "source": [
        "# Supervised segmentation\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxOV87F7CdZF"
      },
      "source": [
        "## Some general imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "LB8qWwc2CdZG"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(style='ticks', palette='Set2')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi83rQkrCdZH"
      },
      "source": [
        "## Predicting who will survive the Titanic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3BFDorkCdZH"
      },
      "source": [
        "This time we will use a clasic introductory dataset that contains demographic and traveling information for the Titanic passengers. The goal is to predict the survival of these passengers. We will only keep a few variables of interest and transform all of them to numeric variables. We will also drop some outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3pjA5qOCdZI"
      },
      "source": [
        "# Load data\n",
        "path = \"https://raw.githubusercontent.com/yizuc/dataminingfall2021/master/Module2_Supervised/data/titanic.csv\"\n",
        "df = pd.read_csv(path)[[\"survived\", \"pclass\", \"sex\", \"age\", \"fare\"]].dropna()\n",
        "# Transform sex column to a numeric variable\n",
        "df[\"female\"] = (df.sex == \"female\").astype(int)\n",
        "df = df.drop(\"sex\", axis=\"columns\")\n",
        "# Drop outliers. This is to help the visualization in the next examples.\n",
        "df = df[df.fare < 400]\n",
        "# Take a look at the data\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw2ZE8ZsCdZJ"
      },
      "source": [
        "We'd like to use information about the passengers to predict whether they will survive. Let's start by taking a look at how well some of the variables \"split\" the data according to our target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-mQeFPKCdZJ"
      },
      "source": [
        "sns.boxplot(\"survived\", \"fare\", width=0.4, data=df)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf4bilLICdZK"
      },
      "source": [
        "Above we see boxplots that shows the fare distribution grouped by our target variable (survival). The left boxplot corresponds to people that died and the right one to people that survived. Alternatively, we could plot the distribution of fare according to survival"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vz2FHTsCdZK"
      },
      "source": [
        "for r in range(2):\n",
        "    hist = df[df.survived == r].hist('fare')\n",
        "    plt.title(\"survived =\" + str(r))\n",
        "    plt.ylim(0,470)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8rUX2GRCdZL"
      },
      "source": [
        "It seems that people that paid less are less likely to survive. We could use this to predict that people that paid more than 50 will survive. How effective is this threshold? Let's quantify it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB75dnwICdZL"
      },
      "source": [
        "***\n",
        "\n",
        "\n",
        "**Entropy** ($H$) and **information gain** ($IG$) au useful tools for measuring the effectiveness of a split on the data. Entropy measures how random data is, information gain is a measure of the reduction in randomness after performing a split.\n",
        "\n",
        "<table style=\"border: 0px\">\n",
        "<tr style=\"border: 0px\">\n",
        "<td style=\"border: 0px\"><img src=\"https://github.com/yizuc/dataminingfall2021/blob/master/Module2_Supervised/images/dsfb_0304.png?raw=1\" height=80% width=80%>\n",
        "Figure 3-4. Splitting the \"write-off\" sample into two segments, based on splitting the Balance attribute (account balance) at 50K.</td>\n",
        "<td style=\"border: 0px; width: 30px\"></td>\n",
        "<td style=\"border: 0px\"><img src=\"https://github.com/yizuc/dataminingfall2021/blob/master/Module2_Supervised/images/dsfb_0305.png?raw=1\" height=75% width=75%>\n",
        "Figure 3-5. A classification tree split on the three-values Residence attribute.</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Given the data, it is fairly straight forward to calculate both of these quantities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or_OsPvbCdZM"
      },
      "source": [
        "##### Functions to get the entropy and IG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "BMyaKAiLCdZM"
      },
      "source": [
        "def entropy(target_column):\n",
        "    \"\"\"\n",
        "        computes -sum_i p_i * log_2 (p_i) for each i\n",
        "    \"\"\"\n",
        "    # get the counts of each target value\n",
        "    target_counts = target_column.value_counts().astype(float).values\n",
        "    total = target_column.count()  \n",
        "    # compute probas\n",
        "    probas = target_counts/total\n",
        "    # p_i * log_2 (p_i)\n",
        "    entropy_components = probas * np.log2(probas)\n",
        "    # return negative sum\n",
        "    return - entropy_components.sum()\n",
        "\n",
        "def information_gain(df, info_column, target_column, threshold):\n",
        "    \"\"\"\n",
        "        computes H(target) - H(target | info > thresh) - H(target | info <= thresh)\n",
        "    \"\"\"\n",
        "    # split data\n",
        "    data_above_thresh = df[df[info_column] > threshold]\n",
        "    data_below_thresh = df[df[info_column] <= threshold]\n",
        "    # get entropy\n",
        "    H = entropy(df[target_column])\n",
        "    entropy_above = entropy(data_above_thresh[target_column])\n",
        "    entropy_below = entropy(data_below_thresh[target_column])\n",
        "    # compute weighted average\n",
        "    ct_above = data_above_thresh.shape[0]\n",
        "    ct_below = data_below_thresh.shape[0]\n",
        "    tot = float(df.shape[0])\n",
        "    return H - entropy_above*ct_above/tot - entropy_below*ct_below/tot "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap2PewRvCdZN"
      },
      "source": [
        "Now that we have a way of calculating $H$ and $IG$, let's test our prior hunch, that using 50 as a split on fare allows us to determine if someone will survive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHs-ejXQCdZN"
      },
      "source": [
        "threshold = 50\n",
        "prior_entropy = entropy(df[\"survived\"])\n",
        "IG = information_gain(df, \"fare\", \"survived\", threshold)\n",
        "print (\"IG of %.4f using a threshold of %.2f given a prior entropy of %.4f\" % (IG, threshold, prior_entropy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TTqnc8qCdZN"
      },
      "source": [
        "How good was our guess of 50? Let's loop through all possible splits on fare and see what is the best!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5XTFVbDXCdZN"
      },
      "source": [
        "def best_threshold(df, info_column, target_column, criteria=information_gain):\n",
        "    maximum_ig = 0\n",
        "    maximum_threshold = 0\n",
        "\n",
        "    for thresh in df[info_column]:\n",
        "        IG = criteria(df, info_column, target_column, thresh)\n",
        "        if IG > maximum_ig:\n",
        "            maximum_ig = IG\n",
        "            maximum_threshold = thresh\n",
        "            \n",
        "    return (maximum_threshold, maximum_ig)\n",
        "\n",
        "maximum_threshold, maximum_ig = best_threshold(df, \"fare\", \"survived\")\n",
        "\n",
        "print (\"the maximum IG we can achieve splitting on weight is %.4f using a thresh of %.2f\" % (maximum_ig, maximum_threshold))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ydJw5VCdZO"
      },
      "source": [
        "Other observed features may also give us a strong clue about survival."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBcP6gR3CdZO"
      },
      "source": [
        "# Names of different columns\n",
        "categorical_cols = [\"pclass\", \"female\"]\n",
        "continuous_cols = [\"age\", \"fare\"]\n",
        "target_col = \"survived\"\n",
        "predictor_cols = categorical_cols + continuous_cols\n",
        "\n",
        "# This is to plot everything in a 2x2 space\n",
        "rows, cols = 2, 2\n",
        "fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(7*cols, 7*rows))\n",
        "axs = axs.flatten()\n",
        "posn = 0\n",
        "\n",
        "# Plot continous features\n",
        "for col in continuous_cols:\n",
        "    sns.boxplot(target_col, col, data=df, ax=axs[posn])\n",
        "    axs[posn].set_ylabel(col)\n",
        "    axs[posn].set_title(\"\")\n",
        "    posn += 1\n",
        "\n",
        "# Plot categorical features\n",
        "for col in categorical_cols:\n",
        "    df.groupby(col)[target_col].mean().plot(kind=\"bar\", ax=axs[posn])\n",
        "    axs[posn].set_ylabel(\"Fraction that survived\")\n",
        "    posn += 1\n",
        "    \n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47TmtMrcCdZO"
      },
      "source": [
        "This now begs the question: what feature gives the most effective split? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU_qoxtNCdZO"
      },
      "source": [
        "def best_split(df, info_columns, target_column, criteria=information_gain):\n",
        "    maximum_ig = 0\n",
        "    maximum_threshold = 0\n",
        "    maximum_column = \"\"\n",
        "    \n",
        "    for info_column in info_columns:\n",
        "        thresh, ig = best_threshold(df, info_column, target_column, criteria)\n",
        "        \n",
        "        if ig > maximum_ig:\n",
        "            maximum_ig = ig\n",
        "            maximum_threshold = thresh\n",
        "            maximum_column = info_column\n",
        "\n",
        "    return maximum_column, maximum_threshold, maximum_ig\n",
        "\n",
        "max_col, max_threshold, max_ig = best_split(df, predictor_cols, \"survived\")\n",
        "\n",
        "print (\"The best column to split on is %s giving us a IG of %.4f using a thresh of %.2f\" % (max_col, max_ig, max_threshold))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OhsNOo-CdZP"
      },
      "source": [
        "### The Classifier Tree: Recursive Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnGecaC-CdZP"
      },
      "source": [
        "Of course, splitting the data one time sometimes isn't enough to make accurate categorical predictions. However, we can continue to split the data recursively, building a tree-structured model that may give better results. This recursive splitting is the basis for a \"decision tree classifier\" or \"classifier tree\", a popular and powerful class of machine learning algorithm. In particular, this specific algorithm is known as ID3 for Iterative Dichotomizer. \n",
        "\n",
        "What are some other ways you might consider splitting the data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuK_zln0CdZP"
      },
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "cmap = {1: 'blue', 0: 'red'}\n",
        "df.plot(kind=\"scatter\", x=\"fare\", y=\"age\", c=[cmap[c] for c in df[target_col]])\n",
        "plt.legend(handles=[mpatches.Patch(color=cmap[k], label=k) for k in cmap], loc=1, title=\"Target\", frameon=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njLmvW2JCdZQ"
      },
      "source": [
        "Rather than build a classifier tree from scratch (think if you could now do this!) let's use sklearn's implementation which includes some additional functionality. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vn1gWDpCdZQ"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Let's define the model (tree)\n",
        "decision_tree = DecisionTreeClassifier(max_depth=3, criterion=\"entropy\")   # Look at those 2 arguments !!! \n",
        "# Let's tell the model what is the data\n",
        "decision_tree.fit(df[predictor_cols], df[\"survived\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWXt6Y0ZCdZR"
      },
      "source": [
        "We now have a classifier tree, let's visualize the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n2877h0CdZR"
      },
      "source": [
        "from IPython.display import Image\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "def visualize_tree(decision_tree, feature_names, class_names, directory=\".\", name=\"tree\",proportion=True):\n",
        "    # Export our decision tree to graphviz format\n",
        "    dot_name = \"%s/%s.dot\" % (directory, name)\n",
        "    dot_file = export_graphviz(decision_tree, out_file=dot_name,\n",
        "                               feature_names=feature_names, class_names=class_names,proportion=proportion)\n",
        "    # Call graphviz to make an image file from our decision tree\n",
        "    image_name = \"%s/%s.png\" % (directory, name)\n",
        "    os.system(\"dot -T png %s -o %s\" % (dot_name, image_name))\n",
        "    # Return the .png image so we can see it\n",
        "    return Image(filename=image_name)\n",
        "\n",
        "visualize_tree(decision_tree, predictor_cols, [\"dies\", \"survives\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPkq60stCdZS"
      },
      "source": [
        "Let's look at `\"age\"` and `\"fare\"`, including the **DECISION SURFACE!!**\n",
        "\n",
        "More details for this graph: [sklearn decision surface](http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXT4_vxpCdZS"
      },
      "source": [
        "def Decision_Surface(data, col1, col2, target, model, probabilities=False):\n",
        "    # Get bounds\n",
        "    x_min, x_max = data[col1].min(), data[col1].max()\n",
        "    y_min, y_max = data[col2].min(), data[col2].max()\n",
        "    # Create a mesh\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.5), np.arange(y_min, y_max,0.5))\n",
        "    meshed_data = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n",
        "    # Get predictions for the mesh\n",
        "    tdf = data[[col1, col2]]\n",
        "    model.fit(tdf, target)\n",
        "    if probabilities:\n",
        "        Z = model.predict_proba(meshed_data)[:, 1].reshape(xx.shape)\n",
        "    else:\n",
        "        Z = model.predict(meshed_data).reshape(xx.shape)                \n",
        "    # Chart details\n",
        "    plt.figure(figsize=[12,7])\n",
        "    plt.title(\"Decision surface\")    \n",
        "    plt.xlabel(col1)\n",
        "    plt.ylabel(col2)\n",
        "    if probabilities:\n",
        "        # Color-scale on the contour (surface = separator)\n",
        "        cs = plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm_r, alpha=0.4)\n",
        "    else:\n",
        "        # Only a curve/line on the contour (surface = separator)\n",
        "        cs = plt.contourf(xx, yy, Z, levels=[-1,0,1], cmap=plt.cm.coolwarm_r, alpha=0.4)\n",
        "    # Plot scatter plot    \n",
        "    cmap = {1: 'blue', 0: 'red'}\n",
        "    colors = [cmap[c] for c in df[target_col]]\n",
        "    plt.scatter(data[col1], data[col2], color=colors)\n",
        "    # Build legend\n",
        "    plt.legend(handles=[mpatches.Patch(color=cmap[k], label=k) for k in cmap], loc=\"best\", title=\"Target\", frameon=True)\n",
        "    plt.show() \n",
        "    \n",
        "tree_depth=3\n",
        "model = DecisionTreeClassifier(max_depth=tree_depth, criterion=\"entropy\")\n",
        "Decision_Surface(df, \"fare\", \"age\", df.survived, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-in6NApqCdZT"
      },
      "source": [
        "How good is our model? Let's compute accuracy, the percent of times where we correctly identified whether a passanger survives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YCpXz96CdZU"
      },
      "source": [
        "from sklearn import metrics\n",
        "print ( \"Accuracy = %.3f\" % (metrics.accuracy_score(decision_tree.predict(df[predictor_cols]), df[\"survived\"])) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIBr_niICdZU"
      },
      "source": [
        "What are some other ways we could classify the data? Last class we used a linear model, let's take a look to see how that partitions the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfh97M8xCdZU"
      },
      "source": [
        "from sklearn import linear_model\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "lin_model = linear_model.LogisticRegression()\n",
        "Decision_Surface(df, \"fare\", \"age\", df.survived, lin_model, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8JehRW0CdZU"
      },
      "source": [
        "And this is the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IKkdyiaCdZV"
      },
      "source": [
        "from sklearn import metrics\n",
        "lin_model.fit(df[predictor_cols], df.survived)\n",
        "print (\"Accuracy = %.3f\" % (metrics.accuracy_score(lin_model.predict(df[predictor_cols]), df.survived)) )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}