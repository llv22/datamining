{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Regularization and learning curves.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu6CDj2jxLEz"
      },
      "source": [
        "# Regularization and Learning Curves\n",
        "\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Kw7wdQxLE1"
      },
      "source": [
        "Import all of the packages we will need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_1ZLv7-xLE2"
      },
      "source": [
        "# Import the libraries we will be using\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = 10, 8\n",
        "\n",
        "# Our custom libraries!\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "! git clone https://github.com/yizuc/datamining.git\n",
        "from datamining.ds_utils.decision_surface import *\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS7XW9jdxLE2"
      },
      "source": [
        "### Complexity control, revisited\n",
        "\n",
        "Recall that one of the most important fundamental principles underlying data science, the basis for machine learning, is *complexity control*.  We must manage the tension between allowing ourselves to fit complex patterns in the data (a good thing), and the tendency to fit idiosyncracies in a particular data set--things that do not generalize (a bad thing).  So, we attempt to control complexity.  One way to control complexity is just not to allow our data mining procedure to fit complex models in the first place.  That's what we do when we build linear models from small sets of features.  But let's consider the case where we think larger feature sets or non-linearities might add important predictive power--so we will attempt to control complexity in a data-driven fashion.\n",
        "\n",
        "We will be using the concrete data (there was a brief example of this data set in Module 1). The data set consists of several compressive strength tests for various types of concrete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4pOKayvxLE3"
      },
      "source": [
        "concrete_url = \"https://www.openml.org/data/get_csv/1762521/phpZGl7F2\"\n",
        "concrete_df = pd.read_csv(concrete_url).dropna()\n",
        "concrete_df.rename(columns={\n",
        "    \"Cement (component 1)(kg in a m^3 mixture)\"             : \"cement\",\n",
        "    \"Blast Furnace Slag (component 2)(kg in a m^3 mixture)\" : \"slag\",\n",
        "    \"Fly Ash (component 3)(kg in a m^3 mixture)\"            : \"fly_ash\",\n",
        "    \"Water  (component 4)(kg in a m^3 mixture)\"             : \"water\",\n",
        "    \"Superplasticizer (component 5)(kg in a m^3 mixture)\"   : \"superplasticizer\",\n",
        "    \"Coarse Aggregate  (component 6)(kg in a m^3 mixture)\"  : \"coarse_agg\",\n",
        "    \"Fine Aggregate (component 7)(kg in a m^3 mixture)\"     : \"fine_agg\",\n",
        "    \"Age (day)\"                                             : \"age\",\n",
        "    \"Concrete compressive strength(MPa. megapascals)\"      : \"strength\"\n",
        "}, inplace= True)\n",
        "\n",
        "concrete_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdZ2LvT1xLE5"
      },
      "source": [
        "Suppose we want to predict which concrete formulas are strong enough for a particular application we have in mind (say 35 megapascals or more). This would be a binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjOhfw8fxLE6"
      },
      "source": [
        "concrete_df[\"is_strong\"] = concrete_df.strength >= 35\n",
        "concrete_df = concrete_df.drop(\"strength\", axis=\"columns\")\n",
        "X = concrete_df.iloc[:, :-1]\n",
        "Y = concrete_df.is_strong"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssJQKL0YxLE6"
      },
      "source": [
        "### Complexity control for logistic regression (regularization)\n",
        "\n",
        "Recall that when fitting a logistic regression classifier, we try to find the set of weights, $\\textbf{w}$, that maximize the fit to the data, based on some fit (objective) function. In this case, let's call our objective function $g()$, which means that we want $\\arg\\max_\\textbf{w} g(\\textbf{x}, \\textbf{w})$.\n",
        "\n",
        "Since we are trying to find the set of weight without too much complexity, when we perform **regularization** we **penalize** our fit as it gets more complex.  This is achieved by adding a \"penalty term\" into the objective function, and using a \"regularization parameter\" $\\lambda$ (also sometimes represented as `c`, which is usually $\\frac{1}{\\lambda}$ so smaller values of `c` lead to larger complexity penalties) to specify how much importance our optimization procedure should place on the fit vs. the penalty:\n",
        "\n",
        "$\\arg\\max_\\textbf{w} g(\\textbf{x}, \\textbf{w}) - \\lambda \\cdot \\text{penalty}(\\textbf{w})$.\n",
        "\n",
        "The two most common type of regularization in logistic regression are the so-called $L_1$ and $L_2$ regularizations, which simply use the sum of the absolute value of the weights (w) and the sum of the squares of the weights, respectively, as the penalty.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raUZiJ78xLE7"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def get_polynomial(degree, model):\n",
        "    # create different powers of X\n",
        "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"model\", model)])\n",
        "    return pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcFYl1eexLE7"
      },
      "source": [
        "# Let's use 3rd degree polynomial\n",
        "degree = 3\n",
        "cs = {\"L2\": range(-16,1, 3), \"L1\": range(-8, 4, 2)} \n",
        "# Plot different regularization values for L1 and L2 regularization\n",
        "position = 0\n",
        "plt.figure(figsize=(15, 20))\n",
        "for regularization in ['L2', 'L1']:    \n",
        "    # Try some regularization values\n",
        "    for i in cs[regularization]:\n",
        "        # Modeling\n",
        "        c = np.power(10.0, i)\n",
        "        model = LogisticRegression(penalty=regularization.lower(), C=c, solver='liblinear')\n",
        "        model = get_polynomial(degree, model)\n",
        "        # Plotting\n",
        "        position += 1\n",
        "        plt.subplot(4, 3, position)\n",
        "        Decision_Surface(X, \"slag\", \"cement\", Y, model)\n",
        "        plt.title(regularization + \", C = \" + str(np.power(10.0, i)))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXXXUSIAxLE8"
      },
      "source": [
        "### Feature's weights with L1/L2  and normalization\n",
        "\n",
        "Let's take a look to the different values of our weights with each type of penalty but first, let's understand **normalization**. \n",
        "\n",
        "What is normalization? Why do we need normalization? Each time we work with data, it is very important to consider the \"scale\" of the features. Some features might have distinct values from 1 to 1000, and other features might have values from 0 to 1. As many different data science/machine learning methods compare data along different dimensions, it can often be important to make sure the dimensions are comparable.\n",
        "\n",
        "To do this re-scaling there are are many approaches, the most common being:\n",
        "\n",
        "- _Normalization_ : we rescale our data so that the features have unit norms  \n",
        "- _Standardization_ : we rescale our data acting as if each features is normally distributed (Gaussian with zero mean and unit variance)\n",
        "- _Scaling to a range_ : we rescale our data based on the minimum and maximum value of each feature \n",
        "\n",
        "\n",
        "( sklearn has a built-in function to help us re-scaling our data -- see below)\n",
        "\n",
        "**Let's take a look at the data before and after re-scaling.**\n",
        "\n",
        "Before re-scaling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb3GHdO5xLE8"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivuUAuRLxLE9"
      },
      "source": [
        "from sklearn.preprocessing import scale\n",
        "X_scaled = pd.DataFrame(scale(X, axis=0, with_mean=True, with_std=True, copy=True), columns=X.columns.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw4dfMAFxLE9"
      },
      "source": [
        "After re-scaling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "984D6FKMxLE9"
      },
      "source": [
        "X_scaled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-jtIdlIxLE9"
      },
      "source": [
        "summary = X_scaled.describe()\n",
        "for column in summary:\n",
        "    summary[column] = summary[column].apply(lambda x: round(x,1))\n",
        "summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e7Yz-IyxLE-"
      },
      "source": [
        "### Visualizing the weights as we change the regularization\n",
        "\n",
        "Before we try and delve into the impact regularization has on the coefficients logistic regression assigns to different features, let's investigate the coefficients before any regularization is performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPw2LYUjxLE-"
      },
      "source": [
        "def get_coeffs(X, Y, model):\n",
        "    model.fit(X, Y)\n",
        "    return dict(zip(X.columns, model.coef_[0]))\n",
        "\n",
        "pd.DataFrame([get_coeffs(X_scaled, Y, LogisticRegression(solver='liblinear'))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxsYkpzYxLE-"
      },
      "source": [
        "Now we can see how the model weights change with differing degrees of complexity control."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfWik4rkxLE_"
      },
      "source": [
        "def get_lr_regularization_paths(X, Y, regtype, reg_values):\n",
        "    coefs = [get_coeffs(X, Y, LogisticRegression(penalty=regtype, C=10**reg, solver='liblinear')) for reg in reg_values]\n",
        "    df = pd.DataFrame(coefs)\n",
        "    df[\"regularization\"] = reg_values\n",
        "    df.set_index(\"regularization\", inplace=True)    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10ptlKnNxLE_"
      },
      "source": [
        "# Go through a bunch of ascending regularization parameters\n",
        "regs = np.arange(-5, 5, 0.5)  \n",
        "# Get coefficients \n",
        "l1_coefs = get_lr_regularization_paths(X_scaled, Y, \"l1\", regs)\n",
        "l1_coefs.plot()\n",
        "plt.title(\"L1 Regularization paths\")\n",
        "l2_coefs = get_lr_regularization_paths(X_scaled, Y, \"l2\", regs)\n",
        "l2_coefs.plot()\n",
        "plt.title(\"L2 Regularization paths\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhB8hRw8xLE_"
      },
      "source": [
        "### Hyper-parameter tuning\n",
        "\n",
        "As seen previously, most models have a number of settings that impact their behavior and therefore generalization performance (such as the regularization settings we have seen today). One common use of cross-validation is to tune these \"hyper-parameters\" (so-called because the coefficients in a model are usually called parameters). One can select several values for a particular hyper-parameter, use cross-validation to estimate model generalization performance, and keep the model that offered the best results.\n",
        "\n",
        "For example, we could try different regularization settings and keep the one that produces the model with the highest accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzgtCv6gxLFA"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "folds = 10\n",
        "\n",
        "def score_model(X, Y, model, scoring=\"accuracy\"):\n",
        "    scores = cross_val_score(model, X, Y, scoring=scoring, cv=folds)\n",
        "    return {\"mean\": scores.mean(), \"std_dev\": scores.std()}\n",
        "\n",
        "reg_vals = np.arange(-4, 2, 0.2)\n",
        "scores_list = [score_model(X_scaled, Y, LogisticRegression(C=10**reg, solver='liblinear')) for reg in reg_vals]\n",
        "\n",
        "accys = np.array([score[\"mean\"] for score in scores_list])\n",
        "# Confidence interval for the mean\n",
        "accys_std = 1.96 * np.array([score[\"std_dev\"] for score in scores_list]) / np.sqrt(folds)\n",
        "\n",
        "plt.plot(reg_vals, accys)\n",
        "plt.fill_between(reg_vals, accys + accys_std, accys - accys_std, alpha=0.3)\n",
        "plt.xlabel(\"Regularization\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Tuning regularization using Cross Validation\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Rj9bnNxLFA"
      },
      "source": [
        "### Grid Search: tuning many hyper-parameters\n",
        "\n",
        "The concept explored above can be extended in a very natural way to simultaniuosly optimize many hyper-parameters using a technique called \"grid search\". One first defines a \"grid\", of hyper-parameter values to explore. The grid search explores all possible combinations of these settings, selecting the setting with the best cross-validated value of the chosen generalization measure.\n",
        "\n",
        "Sklearn provides a convenient implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7WRSv78xLFA"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid = {\"polynomial_features__degree\": [1, 2, 3], \"model__C\": [10.0**c for c in np.arange(-2, 3)]}\n",
        "\n",
        "def Polynomial_Model(degree=1, reg=1):\n",
        "    return get_polynomial(degree=degree, model=LogisticRegression(C=reg, solver='liblinear', max_iter=1000))\n",
        "\n",
        "# gridsearchcv behaves just like a model, with fit, predict, and some additional functionalities\n",
        "tuned_model = GridSearchCV(Polynomial_Model(), grid, scoring=\"accuracy\", cv=10, verbose=1)\n",
        "tuned_model.fit(X_scaled, Y)\n",
        "\n",
        "print (\"Best accuracy: %0.3f, using: \" % tuned_model.best_score_)\n",
        "print (tuned_model.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD2b3keMxLFB"
      },
      "source": [
        "Let's take a look at the performance over our parameter grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTqbmnVwxLFB"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "means = tuned_model.cv_results_['mean_test_score']\n",
        "stds = tuned_model.cv_results_['std_test_score']\n",
        "params = tuned_model.cv_results_['params']\n",
        "\n",
        "degrees = [param[\"polynomial_features__degree\"] for param in params]\n",
        "reg_vals = [np.log10(param[\"model__C\"]) for param in params]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = Axes3D(fig)\n",
        "surf = ax.plot_trisurf(degrees, reg_vals, means, linewidth=0.1, cmap=plt.cm.coolwarm)\n",
        "ax.set_xlabel('degrees')\n",
        "ax.set_ylabel('regularization')\n",
        "ax.set_zlabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0JigIMWxLFC"
      },
      "source": [
        "**A word of caution: ** Grids with many potential values combined with slow-ish model training can really blow up the time it takes to tune a model! \n",
        "\n",
        "Also, note that both of the hyper-parameters tuned above are used to control complexity. This will nearly always be the case when optimizing a ML model-- getting the best generalization performance out of one's model means balancing the tension between allowing a model to fit fine-grained patterns to the data (a good thing), and the tendancy to fit idiosyncracies in a particular data set-- things that don't generalize (a bad thing). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iivqBtMpxLFC"
      },
      "source": [
        "### Learning curves for model assessment\n",
        "\n",
        "Throughout the past lectures we analyzed and compared model accuracies using a unique **sample size** that was essentially fixed and determined by the size of the datasets we were considering. \n",
        "\n",
        "Very often we also want to assess the relationship between how much data we are using to train the models, and the generalization performance we achieve.  For example, do we have a good idea whether we should invest in acquiring more training data? The only way to answer this question is again, experiment with different sample sizes. The main way to do this assessment is via **_learning curves_**: analyze the change of the generalization performance (accuray on the holdout data, in this case) based on different sizes of the training set.\n",
        "\n",
        "What would we expect to see? Holding everything else fixed, the generalization should be better with more training data, up until a certain point. Then, more data won't increase generalization performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuJWHMumxLFC"
      },
      "source": [
        "import sklearn.model_selection as cv\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate_model_on_sample(X, Y, model, pct, scoring=accuracy_score):\n",
        "    kf = cv.StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    scores = []    \n",
        "    for train_index, test_index in kf.split(X, Y):  \n",
        "        # only take a portion of the training\n",
        "        train_size = len(train_index)\n",
        "        sampled_indices = np.random.permutation(train_index)[:int(pct*train_size)]\n",
        "        model.fit(X.iloc[sampled_indices], Y[sampled_indices])\n",
        "        scores.append(scoring(Y[test_index], model.predict(X.loc[test_index])))       \n",
        "    return np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IesUKcoPxLFC"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "pcts = np.linspace(0.01,1,100).tolist()\n",
        "# Get scores for each classifier\n",
        "dt_scores = [evaluate_model_on_sample(X, Y, DecisionTreeClassifier(), pct) for pct in pcts]\n",
        "lr_scores = [evaluate_model_on_sample(X, Y, LogisticRegression(solver='liblinear'), pct) for pct in pcts]\n",
        "# Plot without confidence interval\n",
        "plt.plot(pcts, dt_scores, label=\"Classifier Tree\")\n",
        "plt.plot(pcts, lr_scores, label=\"Logistic Regression\")\n",
        "plt.xlabel(\"Percent of data\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkwvrWuxxLFD"
      },
      "source": [
        "As we can see, learning curves will help to determine at least 2 things:\n",
        "\n",
        "- We can see which model performs better or worse for each sample size (e.g. Decision Tree vs Logistic Regression)\n",
        "- We can get a sense of whether getting more data (or using less) will improve (or not degrade) generalization.\n",
        "\n",
        "Very often, the more data we have, the more \"complex\" models we can use (as shown in the chart above)."
      ]
    }
  ]
}