{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "mining_text.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT8pqjSygJDB"
      },
      "source": [
        "# Dealing with text and Naive Bayes\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58TFIXexgJDF"
      },
      "source": [
        "# Import the libraries we will be using\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# We will want to keep track of some different roc curves, lets do that here\n",
        "tprs = []\n",
        "fprs = []\n",
        "roc_labels = []\n",
        "aucs = []"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSm3z7pygJDH"
      },
      "source": [
        "## Document classification and customer satisfaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxdNbnDzgJDH"
      },
      "source": [
        "You've been hired by Trans American Airlines (TAA) as a business analytics professional. One of the top priorities of TAA is  customer service. For TAA, it is of utmost importance to identify whenever customers are unhappy with the way employees have treated them. You've been hired to analyze twitter data in order to detect whenever a customer has complaints about flight attendants. Tweets suspected to be related to flight attendant complaints should be forwarded directly to the customer service department in order to track the issue and take corrective actions.  \n",
        "\n",
        "Let's start by loading the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97u-r5qTgVsc"
      },
      "source": [
        "! git clone https://github.com/yizuc/datamining.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF7nQCpGgJDH"
      },
      "source": [
        "data_path = '/content/datamining/Module6_Text_NaiveBayes/data/tweets.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt7HbloggJDI"
      },
      "source": [
        "Let's take a look at what do people complain about in Twitter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6VTHCylgJDJ"
      },
      "source": [
        "df.negativereason.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLTn8zS8gJDJ"
      },
      "source": [
        "Our label is given by \"Flight Attendant Complaints\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o73DR-4BgJDK"
      },
      "source": [
        "# We'll call our label 'service_issue' and keep only the text as a feature.\n",
        "df[\"is_fa_complaint\"] = (df.negativereason == \"Flight Attendant Complaints\").astype(int)\n",
        "df = df[[\"is_fa_complaint\", \"text\"]]\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTI2Th1UgJDK"
      },
      "source": [
        "Let's take a look at the percentage of tweets related to complaints about flight attendants."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNm5jCSUgJDL"
      },
      "source": [
        "df['is_fa_complaint'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hxsb_QDgJDL"
      },
      "source": [
        "Here are some examples of the tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ2p098NgJDL"
      },
      "source": [
        "print(\"Flagged as FA complaints:\")\n",
        "print(df[df.is_fa_complaint == 1].text.values[0:5])\n",
        "\n",
        "print(\"\\nNot flagged as FA complaint:\")\n",
        "print(df[df.is_fa_complaint == 0].text.values[0:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5NnNpp4gJDM"
      },
      "source": [
        "Since we are going to do some modeling, we should split our data into a training and a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLOQZKJugJDM"
      },
      "source": [
        "X = df['text']\n",
        "Y = df['is_fa_complaint']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaqRDM-TgJDM"
      },
      "source": [
        "### Text as features\n",
        "How can we turn the large amount of text for each record into useful features?\n",
        "\n",
        "\n",
        "#### Binary representation\n",
        "One way is to create a matrix that uses each word as a feature and keeps track of whether or not a word appears in a document/record. You can do this in sklearn with a `CountVectorizer()` and setting `binary` to `true`. The process is very similar to how you fit a model: you will fit a `CounterVectorizer()`. This will figure out what words exist in your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LD3w2kLgJDM"
      },
      "source": [
        "binary_vectorizer = CountVectorizer(binary=True)\n",
        "binary_vectorizer.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZXobtTMgJDN"
      },
      "source": [
        "Let's look at the vocabulary the `CountVectorizer()` learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDYCj2DggJDN"
      },
      "source": [
        "vocabulary_list = list(zip( binary_vectorizer.vocabulary_.keys(), binary_vectorizer.vocabulary_.values()) )\n",
        "\n",
        "vocabulary_list[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzZ_s7SagJDN"
      },
      "source": [
        "Now that we know what words are in the data, we can transform our text into a clean matrix. Simply .transform() the raw data using our fitted CountVectorizer(). You will do this for the training and test data. What do you think happens if there are new words in the test data that were not seen in the training data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4mv42yzgJDO"
      },
      "source": [
        "X_train_binary = binary_vectorizer.transform(X_train)\n",
        "X_test_binary = binary_vectorizer.transform(X_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzyJm5qDgJDO"
      },
      "source": [
        "We can take a look at our new `X_test_counts`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A79lnj6YgJDO"
      },
      "source": [
        "X_test_binary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gBycBTRgJDO"
      },
      "source": [
        "Sparse matrix? Where is our data?\n",
        "\n",
        "If you look at the output above, you will see that it is being stored in a *sparse* matrix (as opposed to the typical dense matrix) that is ~3k rows long and ~13k columns. The rows here are records in the original data and the columns are words. Given the shape, this means there are ~39m cells that should have values. However, from the above, we can see that only ~46k cells (~0.12%) of the cells have values! Why is this?\n",
        "\n",
        "To save space, sklearn uses a sparse matrix. This means that only values that are not zero are stored! This saves a ton of space! This also means that visualizing the data is a little trickier. Let's look at a very small chunk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPkrFG7wgJDO"
      },
      "source": [
        "# Recall that 13183 is the index for \"you\"\n",
        "X_test_binary[0:20, 13180:13200].todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXa7uEfYgJDP"
      },
      "source": [
        "#### Applying a model\n",
        "Now that we have a ton of features (one for every word!) let's try using a logistic regression model to predict which tweets are about flight attendant complaints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB1m4FL6gJDP"
      },
      "source": [
        "def get_model_roc(models, Xs_test, names, Y_test):\n",
        "    plt.rcParams['figure.dpi'] = 100\n",
        "    for i in range(len(models)):\n",
        "        model = models[i]\n",
        "        X_test = Xs_test[i]\n",
        "        name = names[i]\n",
        "        probs = model.predict_proba(X_test)[:,1]\n",
        "        fpr, tpr, thresholds = metrics.roc_curve(Y_test, probs)\n",
        "        plt.plot(fpr, tpr, label=name)\n",
        "        plt.plot([0, 1], [0, 1], linestyle='dashed', color='black')\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(\"ROC Curve\")\n",
        "        print (\"AUC for {0} = {1:.3f}\".format(name, metrics.roc_auc_score(Y_test, probs)))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "        \n",
        "model_binary = LogisticRegression(solver='liblinear')\n",
        "model_binary.fit(X_train_binary, Y_train)\n",
        "get_model_roc([model_binary], [X_test_binary], ['binary'], Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKA4nSRXgJDP"
      },
      "source": [
        "#### Counts instead of binary\n",
        "Instead of using a 0 or 1 to represent the occurence of a word, we can use the actual counts. We do this the same way as before, but now we leave `binary` set to `false` (the default value)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gbQQI42gJDP"
      },
      "source": [
        "# Fit a counter\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_vectorizer.fit(X_train)\n",
        "\n",
        "# Transform to counter\n",
        "X_train_counts = count_vectorizer.transform(X_train)\n",
        "X_test_counts = count_vectorizer.transform(X_test)\n",
        "\n",
        "# Model\n",
        "model_counts = LogisticRegression(solver='liblinear')\n",
        "model_counts.fit(X_train_counts, Y_train)\n",
        "\n",
        "get_model_roc([model_binary, model_counts], [X_test_binary, X_test_counts], ['binary', 'counts'], Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtaUnpM4gJDQ"
      },
      "source": [
        "#### Tf-idf\n",
        "Another popular technique when dealing with text is to use the term frequency - inverse document frequency (tf-idf) measure instead of just counts as the feature values (see the book)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4I0FcfZgJDQ"
      },
      "source": [
        "# Fit a counter\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(X_train)\n",
        "\n",
        "# Transform to a counter\n",
        "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Model\n",
        "model_tfidf = LogisticRegression(solver='liblinear')\n",
        "model_tfidf.fit(X_train_tfidf, Y_train)\n",
        "\n",
        "get_model_roc([model_binary, model_tfidf], [X_test_binary, X_test_tfidf], ['binary', 'tf-idf'], Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGTHAUWugJDQ"
      },
      "source": [
        "The `CountVectorizer()` and `TfidfVectorizer()` functions have many options. You can restrict the words you would like in the vocabulary. You can add n-grams. You can use stop word lists. Which options you should use generally depend on the type of data you are dealing with. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqaJkMaEgJDQ"
      },
      "source": [
        "# Fit a counter\n",
        "ngram_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
        "ngram_vectorizer.fit(X_train)\n",
        "\n",
        "# Transform to a counter\n",
        "X_train_ngram = ngram_vectorizer.transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "# Model\n",
        "model_ngram = LogisticRegression(solver='liblinear')\n",
        "model_ngram.fit(X_train_ngram, Y_train)\n",
        "\n",
        "get_model_roc([model_binary, model_ngram], [X_test_binary, X_test_ngram], ['binary', '2-ngram'], Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWdFiLoXgJDQ"
      },
      "source": [
        "### Modeling with another technique: Naive Bayes\n",
        "\n",
        "So far we have been exposed to tree classifiers and logistic regression in class. Now, it's time for another popular modeling technique of supervised learning (especially in text classification): the Naive Bayes (NB) classifier. In particular, we are using a Bernoulli Naive Bayes (BNB) for our binary classification. (Bernoulli NB is the model described in the book; there are other versions of NB.)\n",
        "\n",
        "As described in your text, the Naive Bayes model is a **probabilistic approach which assumes conditional independence between features** (in this case, each word is a feature, the conditioning is on the true class). It assigns class labels (e.g. service_issue = 1 or service_issue = 0). In other words, Naive Bayes models the probabilities of the presence of each _word_, given that we have a service issue, and given that we do not have a service issue.  Then it combines them using Bayes Theorem (again, as described in the book).\n",
        "\n",
        "Using this model in sklearn works just the same as the others we've seen ([More details here..](http://scikit-learn.org/stable/modules/naive_bayes.html))\n",
        "\n",
        "- Choose the model\n",
        "- Fit the model (Train)\n",
        "- Predict with the model (Train or Test or Use data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ossqe7cqgJDR"
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "model_nb = BernoulliNB()\n",
        "model_nb.fit(X_train_binary, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lp4r0pKgJDR"
      },
      "source": [
        "The past few weeks we have seen that many of the models we are using have different complexity control parameters that can be tweaked. In naive Bayes, the parameter that is typically tuned is the Laplace smoothing value **`alpha`**.\n",
        "\n",
        "Also, there are other versions of naive Bayes:\n",
        "\n",
        "1. **Multinomial naive Bayes (MNB):** This model handles count features and not just binary features. Sometimes MNB is used with binary presence/absence variables anyway (like word presence), even though that violates the model assumptions, because in practice it works well anyway.\n",
        "2. **Gaussian Naive Bayes (GNB):** This model considers likelihood of the features as Gaussian--and thus we can use it for continuous features.  Sometimes GNB and Bernoulli NB are combined when one has features of mixed types.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKOeqGxGgJDR"
      },
      "source": [
        "get_model_roc([model_binary, model_nb], [X_test_binary, X_test_binary], ['binary', 'naive-bayes'], Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}